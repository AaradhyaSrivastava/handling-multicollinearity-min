# Handling Multicollinearity — README

This repository demonstrates how to diagnose and mitigate **multicollinearity** in regression models. It provides a reproducible workflow that generates synthetic data with deliberately correlated features, applies statistical diagnostics, evaluates different regression approaches, and visualizes results. The project is designed as a professional portfolio artifact and can serve as reference code for academic or industry applications.

---

## Project Overview

Multicollinearity occurs when independent variables in a regression model are highly correlated. This leads to unstable parameter estimates, inflated variances, and unreliable interpretations. The purpose of this project is to:

1. **Simulate collinear data** to replicate real-world conditions where predictors are interdependent.
2. **Apply diagnostic techniques** such as Variance Inflation Factor (VIF) and Condition Number to detect multicollinearity.
3. **Fit multiple regression models** and compare their performance:

   * Ordinary Least Squares (OLS) as a baseline.
   * Ridge Regression (L2 regularization) to stabilize coefficients.
   * Lasso Regression (L1 regularization) for feature selection.
   * Principal Component Regression (PCR) to eliminate collinearity through orthogonal components.
4. **Evaluate models** using R² and RMSE metrics.
5. **Visualize results** to highlight coefficient instability in OLS versus stability achieved by Ridge and PCR.

---

## Importance of the Project

This project demonstrates the ability to handle a fundamental challenge in applied statistics and machine learning. The outcomes are relevant in multiple domains:

* **Data Science**: Ensures robustness and interpretability of regression-based predictive models.
* **Finance and Economics**: Addresses collinearity between macroeconomic indicators or financial ratios that can distort forecasts.
* **Engineering and Sensor Analytics**: Stabilizes parameter estimation when sensor variables are correlated.
* **Machine Learning Practice**: Highlights why regularization and dimensionality reduction are essential in real-world workflows.

---

## Features Implemented

* Synthetic dataset generation with controlled collinearity.
* Computation of VIF and Condition Number for diagnostics.
* Model training with OLS, Ridge, Lasso, and PCR.
* Comparative performance evaluation using cross-validation.
* Visualization of coefficient paths across models.
* Single-file minimal implementation for rapid demonstration.

---

## Sample Results

**Diagnostics:**

* VIF values exceeding 150, confirming severe multicollinearity.
* Condition number \~2500–2800, indicating near-singularity of the design matrix.

**Model Performance:**

* Ridge Regression improves R² by approximately 0.05–0.12 compared to OLS.
* PCR achieves stable predictions with reduced dimensionality.
* Lasso performs variable selection but may arbitrarily exclude correlated features.

**Visualization:**

* Coefficient stability plots show OLS coefficients fluctuate sharply, while Ridge and PCR produce consistent estimates.

---


## Repository Structure

* `requirements.txt` — Dependency file.
* `run.py` — Single script to generate data, run diagnostics, train models, and produce outputs.
* `coefficients.png` — Coefficient comparison plot generated by the script.

---

